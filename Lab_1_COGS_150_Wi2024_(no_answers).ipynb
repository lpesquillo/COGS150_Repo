{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lpesquillo/COGS150_Repo/blob/main/Lab_1_COGS_150_Wi2024_(no_answers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ab835a",
      "metadata": {
        "id": "97ab835a"
      },
      "source": [
        "# COGS 150: Lab 1\n",
        "\n",
        "**WRITE YOUR NAME HERE**\n",
        "\n",
        "*Spring 2025*\n",
        "\n",
        "This lab is all about $n$-gram language models. Broadly, we'll cover the following concepts:\n",
        "\n",
        "- What are $n$-grams?  \n",
        "- Building a simple $n$-gram model.  \n",
        "- Using an $n$-gram model to calculate **surprisal**.\n",
        "- Using a language model to **generate** text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this lab:\n",
        "\n",
        "- First, **make a copy**. Click \"File\" --> \"Save a copy in Drive\", then open up and use that version instead.  \n",
        "- Then, work through the different `code` and *text* cells in this notebook. To modify a cell, *double-click* it. (Try not to delete existing code, as that could cause errors.)\n",
        "- Once you're done responding to all the questions, **save the notebook as a PDF** and upload it to your Canvas assignment."
      ],
      "metadata": {
        "id": "KgIuxIcI6PlR"
      },
      "id": "KgIuxIcI6PlR"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5KtN9pAP3nJ",
        "outputId": "7dc06ceb-4cfc-4574-f3fa-44491abc352c"
      },
      "id": "o5KtN9pAP3nJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f43c8db",
      "metadata": {
        "id": "7f43c8db"
      },
      "source": [
        "## Part 1: Introducing $n$-grams\n",
        "\n",
        "> An $n$-gram is an adjacent sequence of length $n$ of characters or words in a corpus of text.\n",
        "\n",
        "In this section, you'll learn how to extract $n$-grams. There are a few components to this process:\n",
        "\n",
        "- **Tokenizing** the text: This means identifying all the words, as well as where each sentence starts and ends.  \n",
        "- **Extracting $n$-grams**: given a list of all the tokens in a corpus, identify all the sequences of length $n$.  \n",
        "- **Applying** these components to a larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ae2841",
      "metadata": {
        "id": "71ae2841"
      },
      "outputs": [],
      "source": [
        "### For making visualizations\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982a5aef",
      "metadata": {
        "id": "982a5aef"
      },
      "source": [
        "### 1a. Tokenizing\n",
        "\n",
        "There are many ways to **tokenize** a corpus. Here, we'll opt for a simple approach, which gets rid of all unwanted punctuation (e.g., commas), identifies all the words, and also identifies the *beginning* and *end* of each sentence.\n",
        "\n",
        "- Each unique word token will be represented as an item in a Python list.  \n",
        "- The beginning and end of a sentence will be represented as `<s>` and `</s>`, respectively.\n",
        "\n",
        "**Reflection**: If we're buliding a model of language, why is it useful to identify where sentences tend to begin and end?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c69bce",
      "metadata": {
        "id": "f4c69bce"
      },
      "outputs": [],
      "source": [
        "small_corpus = \"The cat chased the mouse. The mouse hid in the wall. The cat could not find the mouse.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a112d888",
      "metadata": {
        "id": "a112d888"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Split the text into sentences\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "\n",
        "    # Tokenize each sentence and add start (<s>) and stop (</s>) tokens\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        # Add the start token\n",
        "        tokens.append('<s>')\n",
        "        # Split the sentence into words by whitespace and remove non-alphanumeric characters, then add to tokens\n",
        "        tokens.extend(re.findall(r'\\b\\w+\\b', sentence.lower()))\n",
        "        # Add the stop token\n",
        "        tokens.append('</s>')\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7371b1e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7371b1e3",
        "outputId": "0ca61fda-d6bd-47e2-9b00-91aeaf70c295"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'the', 'man', 'walked', 'to', 'the', 'store', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "### Here's an example of how this works.\n",
        "tokenize(\"The man walked to the store.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763e6aed",
      "metadata": {
        "id": "763e6aed"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "Write a line of code that calls \"tokenize\" on \"small_corpus\". Call the result `tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c418ad",
      "metadata": {
        "id": "82c418ad"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65d6783c",
      "metadata": {
        "id": "65d6783c"
      },
      "source": [
        "### 1b. Extract (and count) $n$-grams\n",
        "\n",
        "To get a better sense of what an $n$-gram is, we'll first look at a function for identifying and counting all the $n$-grams in a big corpus of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00f7728",
      "metadata": {
        "id": "b00f7728"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def extract_ngrams(tokens, n):\n",
        "    \"\"\"Identifies sequences of length n in tokens, and counts how many times it occurs.\"\"\"\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    return Counter([\" \".join(ngram) for ngram in ngrams])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdafc34",
      "metadata": {
        "id": "5cdafc34"
      },
      "outputs": [],
      "source": [
        "### Here's an example of how this function works\n",
        "ngrams = extract_ngrams(tokens, 2)\n",
        "len(ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53dda38b",
      "metadata": {
        "id": "53dda38b"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. How many unique $n$-grams of length 2 are in `small_corpus`? (Hint: use `len` on `ngrams`.)\n",
        "2. What is the most common $n$-gram? (Hint: use `ngrams.most_common()`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731b1ae6",
      "metadata": {
        "id": "731b1ae6"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb802c62",
      "metadata": {
        "id": "cb802c62"
      },
      "source": [
        "### 1c. Apply this to a larger corpus.\n",
        "\n",
        "Now, let's apply this to a much larger **corpus** of text: the book *Emma*, by Jane Austen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8cee47",
      "metadata": {
        "id": "af8cee47"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "emma = ' '.join(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma = emma.replace(\"Mr .\", \"Mr\").replace(\"Mrs .\", \"Mrs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e99f0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e99f0e",
        "outputId": "601d448d-9cc8-4c37-dcd6-95acff5ec489"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172495"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "emma_tokens = tokenize(emma)\n",
        "len(emma_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525f2b4c",
      "metadata": {
        "id": "525f2b4c"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. Use `extract_ngrams` to identify all n-grams of length $2$ from `emma_tokens`.\n",
        "2. How many are there? (Use `len`.)\n",
        "3. What is the most common n-gram? What about the second most common? (Use `ngrams.most_common()`.)  \n",
        "4. Now use `extract_ngrams` to identify all n-grams of length $3$. How many are there? Which is most common?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c620ff0",
      "metadata": {
        "id": "3c620ff0"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b02c936",
      "metadata": {
        "id": "2b02c936"
      },
      "source": [
        "## Part 2: Building a simple $n$-gram model\n",
        "\n",
        "> An **n-gram language model** is a statistical language model, which assigns a probability to some word $w$ as a function of the $(n-1)$ words preceding $w$. For a bigram model, then, this could be written as: $p(w_i | w_{i-1})$\n",
        "\n",
        "\n",
        "We'll break this down into steps:\n",
        "\n",
        "1. Theoretical foundations.  \n",
        "2. Building a simple *bigram* model.  \n",
        "3. Generalizing to an $n$-gram model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27317cd",
      "metadata": {
        "id": "c27317cd"
      },
      "source": [
        "### 2a: Theoretical foundations\n",
        "\n",
        "We want to estimate: $p(w_i | w_{i-1})$\n",
        "\n",
        "Usually, this **conditional probability** is based on the number of times word $w_i$ occurs in a given context, relative to the number of times that *context* appears.\n",
        "\n",
        "For a bigram model, we could write this as follows:\n",
        "\n",
        "$p(w_i | w_{i-1}) = \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})}$\n",
        "\n",
        "For example, $p(dog|the)$ would be calculated by dividing the number of times \"the dog\" occurs by the number of times \"the\" occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8e88aa",
      "metadata": {
        "id": "fd8e88aa"
      },
      "source": [
        "### 2b: Build a *bigram* model.  \n",
        "\n",
        "Now let's build a **bigram** model. To represent our $n$-grams, we'll use a nested dictionary structure that looks something like this:\n",
        "\n",
        "```python\n",
        "{'the':\n",
        "     {'dog': 5,\n",
        "      'cat': 5,\n",
        "      'person': 10}\n",
        "}\n",
        "```\n",
        "\n",
        "Here, each number represents the number of times that word (e.g., \"dog\") occurs after the word `\"the\"`. Those numbers could be converted to probabilities by dividing them by the *sum* of their values.\n",
        "\n",
        "```python\n",
        "{'the':\n",
        "     {'dog': .25,\n",
        "      'cat': .25,\n",
        "      'person': .5}\n",
        "}\n",
        "```\n",
        "\n",
        "The function below builds a bigram model for you. Take a look at the function and see if you can figure out what it's doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b18c727",
      "metadata": {
        "id": "7b18c727"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_bigram_model(tokens):\n",
        "    model = defaultdict(lambda: defaultdict(int))\n",
        "    for i in range(len(tokens)-1):\n",
        "        # Get the n-gram and the word following it\n",
        "        ngram = tuple(tokens[i:i+1])\n",
        "        next_word = tokens[i+1]\n",
        "        model[ngram][next_word] += 1\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b908bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88b908bc",
        "outputId": "ccb3ab89-d668-473b-c83b-699ec043f521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.build_bigram_model.<locals>.<lambda>()>,\n",
              "            {('<s>',): defaultdict(int, {'the': 3}),\n",
              "             ('the',): defaultdict(int, {'cat': 2, 'mouse': 3, 'wall': 1}),\n",
              "             ('cat',): defaultdict(int, {'chased': 1, 'could': 1}),\n",
              "             ('chased',): defaultdict(int, {'the': 1}),\n",
              "             ('mouse',): defaultdict(int, {'</s>': 2, 'hid': 1}),\n",
              "             ('</s>',): defaultdict(int, {'<s>': 2}),\n",
              "             ('hid',): defaultdict(int, {'in': 1}),\n",
              "             ('in',): defaultdict(int, {'the': 1}),\n",
              "             ('wall',): defaultdict(int, {'</s>': 1}),\n",
              "             ('could',): defaultdict(int, {'not': 1}),\n",
              "             ('not',): defaultdict(int, {'find': 1}),\n",
              "             ('find',): defaultdict(int, {'the': 1})})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tokens = tokenize(small_corpus)\n",
        "bigram_model = build_bigram_model(tokens)\n",
        "bigram_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81361f94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81361f94",
        "outputId": "7873d770-435f-4d80-ba1f-9f4375fc8ddb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'cat': 2, 'mouse': 3, 'wall': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "### index into the model like so\n",
        "bigram_model[('the',)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430f47fc",
      "metadata": {
        "id": "430f47fc"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. What are all the words that occur after the word `the`?  \n",
        "2. What is the probability: $p(cat | the)$?\n",
        "3. What about $p(mouse | the)$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553f48c3",
      "metadata": {
        "id": "553f48c3"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da45830d",
      "metadata": {
        "id": "da45830d"
      },
      "source": [
        "### 2c: Build an *n-gram* model.  \n",
        "\n",
        "Now let's build a more general $n$-gram model. Unlike the bigram model, this will allow us to represent contexts of arbitrary length `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb79bb4",
      "metadata": {
        "id": "8cb79bb4"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(tokens, n):\n",
        "    model = defaultdict(lambda: defaultdict(int))\n",
        "    for i in range(len(tokens)-(n-1)):\n",
        "        # Get the n-gram and the word following it\n",
        "        ngram = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "        model[ngram][next_word] += 1\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d035e12a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d035e12a",
        "outputId": "30123824-71fd-440c-eb27-516ec7e1a848"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.build_ngram_model.<locals>.<lambda>()>,\n",
              "            {('<s>', 'the'): defaultdict(int, {'cat': 2, 'mouse': 1}),\n",
              "             ('the', 'cat'): defaultdict(int, {'chased': 1, 'could': 1}),\n",
              "             ('cat', 'chased'): defaultdict(int, {'the': 1}),\n",
              "             ('chased', 'the'): defaultdict(int, {'mouse': 1}),\n",
              "             ('the', 'mouse'): defaultdict(int, {'</s>': 2, 'hid': 1}),\n",
              "             ('mouse', '</s>'): defaultdict(int, {'<s>': 1}),\n",
              "             ('</s>', '<s>'): defaultdict(int, {'the': 2}),\n",
              "             ('mouse', 'hid'): defaultdict(int, {'in': 1}),\n",
              "             ('hid', 'in'): defaultdict(int, {'the': 1}),\n",
              "             ('in', 'the'): defaultdict(int, {'wall': 1}),\n",
              "             ('the', 'wall'): defaultdict(int, {'</s>': 1}),\n",
              "             ('wall', '</s>'): defaultdict(int, {'<s>': 1}),\n",
              "             ('cat', 'could'): defaultdict(int, {'not': 1}),\n",
              "             ('could', 'not'): defaultdict(int, {'find': 1}),\n",
              "             ('not', 'find'): defaultdict(int, {'the': 1}),\n",
              "             ('find', 'the'): defaultdict(int, {'mouse': 1})})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tokens = tokenize(small_corpus)\n",
        "trigram_model = build_ngram_model(tokens, 3)\n",
        "trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252af58e",
      "metadata": {
        "id": "252af58e"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. Calculate the probability: $p(cat|<s>, the)$. How does this compare to $p(cat|the)$ from earlier?\n",
        "2. Calculate the probability: $p(mouse|<s>, the)$. How does this compare to $p(mouse|the)$ from earlier?\n",
        "3. Why do you think these probabilities are different from the bigram model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb99ff25",
      "metadata": {
        "id": "fb99ff25"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1871d045",
      "metadata": {
        "id": "1871d045"
      },
      "source": [
        "## Part 3: Calculating *surprisal*\n",
        "\n",
        "> **Surprisal** is defined as the negative log probability of an event. This term comes from [information theory](https://en.wikipedia.org/wiki/Information_content), and measures the \"unexpectedness\" of an event.\n",
        "\n",
        "Surprisal is defined as follows: $Surprisal(x) = -log_2(p(x))$. For language models, surprisal ends up being a really useful way to *evaluate* the model, and also *measure* how likely different words are in different contexts.\n",
        "\n",
        "In this section, we'll:\n",
        "\n",
        "- Learn about *bits*.  \n",
        "- Implement and use a function to calculate the surprisal of a word in context, from an n-gram model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823b525e",
      "metadata": {
        "id": "823b525e"
      },
      "source": [
        "### 3a: What are \"bits\"?\n",
        "\n",
        "Surprisal is usually measured in $log_2$.\n",
        "\n",
        "This is because information theory is interested in [bits](https://en.wikipedia.org/wiki/Bit): a logical state with two possible values (`1` vs. `0`). You can think of a *bits* as measuring the number of binary coin flips you'd need to arrive at a certain outcome. E.g., for an event of probability $p = 0.5$, you only need to flip a coin once to determine the outcome.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b90be4",
      "metadata": {
        "id": "e3b90be4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def surprisal(p):\n",
        "    return -math.log2(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cb8120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cb8120",
        "outputId": "7aee5b9a-5350-455f-bfd4-8c9287e14496"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "surprisal(.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3688b901",
      "metadata": {
        "id": "3688b901"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. What is the `surprisal` of $p = 0.25$?  \n",
        "2. What about $p = 0.1$?\n",
        "3. Which is larger? What does that tell us about how `surprisal` relates to probability?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23855867",
      "metadata": {
        "id": "23855867"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e28146f",
      "metadata": {
        "id": "5e28146f"
      },
      "source": [
        "### 3b: Surprisal and n-gram models\n",
        "\n",
        "In this section, we define a new function, called `calculate_surprisal`. Given a trained n-gram `model`, a `context`, and a `word`, this function calculates the probability of `word` given the `context`.\n",
        "\n",
        "- Take a moment to look through the function and see if you can understand how it works.\n",
        "- The cell below the function contains some examples of the function in action. Feel free to modify these or write your own to experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965dff71",
      "metadata": {
        "id": "965dff71"
      },
      "outputs": [],
      "source": [
        "def calculate_surprisal(model, context, word):\n",
        "    # Calculate the probability of the word given the context\n",
        "    # In a bigram model, the context is just the previous word\n",
        "    # In an n-gram model, the context is the previous n-1 words\n",
        "    context = tuple(context)\n",
        "    if context in model and word in model[context]:\n",
        "        # Calculate the probability of the word given the context\n",
        "        word_count = model[context][word]\n",
        "        total_count = sum(model[context].values())\n",
        "        probability = word_count / total_count\n",
        "        # Calculate the surprisal\n",
        "        surprisal = -math.log2(probability)\n",
        "    else:\n",
        "        # If the context or word is not found, the surprisal is infinite\n",
        "        surprisal = -float('inf')\n",
        "    return surprisal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc2d1bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbc2d1bc",
        "outputId": "b0694905-ff1b-4f98-cb22-396a639a18cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5849625007211563\n",
            "0.5849625007211563\n",
            "-inf\n"
          ]
        }
      ],
      "source": [
        "## Example\n",
        "print(calculate_surprisal(trigram_model, ('<s>', 'the'), 'mouse'))\n",
        "print(calculate_surprisal(trigram_model, ('<s>', 'the'), 'cat'))\n",
        "print(calculate_surprisal(trigram_model, ('<s>', 'the'), 'dog'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987537f0",
      "metadata": {
        "id": "987537f0"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. Why is the surprisal of \"mouse\" higher than \"cat\"?  \n",
        "2. Why is the surprisal of \"dog\" `-inf`?\n",
        "3. What technique discussed in class would address the issue of a surprisal of `-inf`?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6f26d0",
      "metadata": {
        "id": "ef6f26d0"
      },
      "source": [
        "## Part 4: Generating Text\n",
        "\n",
        "In this section, we'll use our n-gram models to **generate** text.\n",
        "\n",
        "This process typically works as follows:\n",
        "\n",
        "- First, **fit** an n-gram model to a corpus.  \n",
        "- Then, **seed** the n-gram model with a start character (e.g., `<s>`).  \n",
        "- Select the most likely next token, given that start character.  \n",
        "- Continue this process until you've generated either the desired number of tokens or generated an end-of-sentence character (e.g., `</s>`).\n",
        "\n",
        "Because this will involve more complex functions, we'll move away from *custom* functions and we'll use functions from an existing library called `nltk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ece1154",
      "metadata": {
        "id": "1ece1154"
      },
      "outputs": [],
      "source": [
        "### Libraries to import\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8663178",
      "metadata": {
        "id": "a8663178"
      },
      "source": [
        "### 4a. Fitting bigram model.\n",
        "\n",
        "In this section, we fit a bigram model using the `Laplace` and `fit` functions.\n",
        "\n",
        "For more details, check out the [`nltk.lm` package documentation](https://www.nltk.org/api/nltk.lm.html). Try to understand what each code block is doing below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddfaaf70",
      "metadata": {
        "id": "ddfaaf70"
      },
      "outputs": [],
      "source": [
        "# Generate padded bigrams and vocabulary for training data\n",
        "n = 2  # Bigram model\n",
        "train_data, vocab = padded_everygram_pipeline(n, [emma_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55eb7cc",
      "metadata": {
        "id": "e55eb7cc"
      },
      "outputs": [],
      "source": [
        "# Build and train the bigram model\n",
        "model = Laplace(n)\n",
        "model.fit(train_data, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4efbee",
      "metadata": {
        "id": "6b4efbee"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. What does `Laplace` refer to, and why is it used?\n",
        "2. What other related techniques did we discuss in class?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c889f6e",
      "metadata": {
        "id": "4c889f6e"
      },
      "source": [
        "### 4b. Generating text\n",
        "\n",
        "We can now *generate* text using this fit model, using:\n",
        "\n",
        "```\n",
        "model.generate(num_words)\n",
        "```\n",
        "\n",
        "As you'll see, we can also add a `text_seed` to specify which word we want to start with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0830557f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0830557f",
        "outputId": "16a9af99-2b67-486d-81c0-ecf7ec745f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "his guests at the direct the visitors were full as\n"
          ]
        }
      ],
      "source": [
        "### No text seed\n",
        "num_tokens = 10\n",
        "tokens = model.generate(num_tokens)\n",
        "print(' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = model.generate(num_tokens, text_seed = ['this'])\n",
        "print(' '.join(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDTClwYcRlfX",
        "outputId": "87c85ba4-e0c2-4029-b510-04e2dcf5b909"
      },
      "id": "NDTClwYcRlfX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kind in love thought only regret the attraction </s> <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQbF6cZESRjM"
      },
      "id": "tQbF6cZESRjM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions\n",
        "\n",
        "1. What do you think about the text the model is generating? Totally random? Sensible at all?\n",
        "2. Try fitting the model object to different values of $n$. Qualitatively, do you notice any differences between, say, a unigram model ($n = 1$) and a trigram model ($n = 3$)?\n",
        "\n",
        "**Note**: To refit the model, you'll need to rerun these lines:\n",
        "\n",
        "```\n",
        "train_data, vocab = padded_everygram_pipeline(n, [emma_tokens])\n",
        "model = Laplace(n) ### Where n = the desired n-gram model\n",
        "model.fit(train_data, vocab)\n",
        "```"
      ],
      "metadata": {
        "id": "LKqz_D3USa_7"
      },
      "id": "LKqz_D3USa_7"
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code here"
      ],
      "metadata": {
        "id": "NMUNuRRyS2ri"
      },
      "id": "NMUNuRRyS2ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final reflections\n",
        "\n",
        "Now that you've learned a little more about how $n$-gram models work (and how to build simple versions in Python): do you think an $n$-gram model \"understands\" language? Why or why not?"
      ],
      "metadata": {
        "id": "daFrkR1AStgx"
      },
      "id": "daFrkR1AStgx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}